# AI Routing & UX Playbook (Production-Ready)

Last Updated: 2025-10-10
Status: Consolidated and Implementation-Ready

Purpose: This playbook consolidates and supersedes the following: docs/Unified_AI_Implementation_and_UX_Flow.md, docs/Operations_Runbook_Current_vs_Future.md, and all items in docs/archived/*. It clearly differentiates what is CURRENT vs RECOMMENDED, and integrates the implementation guide, strategy, and UX flow in one place.

‚Äî This document is now the single source of truth for AI routing, RAG, UX behavior, and rollout. ‚Äî

## 1) Models & Routing

### CURRENT (as of 2025-10-10)

**Model Assignments:**
- Default model: google/gemini-2.5-flash (balanced performance/cost)
- Vision + reasoning: google/gemini-2.5-pro (multimodal capabilities)
- Heavy analytical/comparatives: gemini-2.5-pro; optionally openai/gpt-5 for premium cases
- Fast summaries/classification: gemini-2.5-flash-lite (optimized for speed)
- Image generation: google/gemini-2.5-flash-image-preview

**Task-Specific Model Selection:**
- General conversation: gemini-2.5-flash
- Domain-specific (healthcare/clinical): gemini-2.5-pro (better accuracy)
- Code generation: gemini-2.5-flash (adequate for most cases)
- Creative writing: gemini-2.5-pro (better nuance)
- Analytical reasoning: openai/gpt-5 or gemini-2.5-pro
- Multilingual: All Gemini models support multilingual

**Context Window Management:**
- Gemini models: Up to 2M tokens (practical limit: use model defaults)
- Current implementation: Rolling trimmed history without summarization
- No adaptive context window yet

**Model Switching Logic Priority:**
1. User explicit selection (always honored)
2. Domain requirement (healthcare ‚Üí Pro; tech ‚Üí Flash)
3. Query complexity & cost target
4. Availability/rate-limit status
5. Default fallback: gemini-2.5-flash

**Integration:**
- Single system prompt enforced in Edge Function (ai-universal-processor)
- All requests through Lovable AI Gateway
- Streaming SSE supported; no secrets on client

### RECOMMENDED (Target)

**Intelligent Model Selection:**
- Auto-upshift when confidence < threshold (e.g., 0.7)
- Task-specific routing matrix (code ‚Üí Flash, creative ‚Üí Pro, analysis ‚Üí GPT-5)
- Cost-aware SLM fallback for high-volume FAQ
- Multi-model orchestration for comparison scenarios

**Context & Memory:**
- Adaptive context window with summarized history and deduped context
- Context window optimization per model type
- Better reasoning with extended context for complex queries

**Infrastructure:**
- Multi-region endpoints and provider redundancy for availability
- Central model map with versioning (created_at/updated_at)
- Fallback chains: Primary ‚Üí Secondary ‚Üí Lite

**Consistency:**
- Unified system prompt across models
- Tone and personality alignment
- Response format standardization

## 2) Context, Memory, and Tokens

### CURRENT
- Use model defaults; maintain rolling trimmed history
- Fallback path on error/latency: downgrade to flash-lite with notice
- Fixed 4000 token limit (not adaptive)

### RECOMMENDED
- Adaptive windowing (summaries of history, deduped context)
- Token-aware routing caps by plan tier
- Dynamic token allocation:
  - Simple queries: 1500 tokens
  - Medium queries: 3000 tokens
  - Complex queries: 6000 tokens
- Context compression for large conversations

## 3) Operational Factors

### CURRENT

**Latency:**
- Prefer Flash by default (1.2s P95 TTFB target)
- Parallel RAG fetch to minimize wait
- Show typing indicator & model badge during streaming
- Pro model: 2.5s P95 TTFB target

**Costs:**
- Prefer Gemini 2.5 Flash/Pro (free until 2025-10-13)
- Display estimated token budgets in UI
- No cost optimization per tier yet
- Fixed 4000 token limit (not adaptive)

**Throughput & Rate Limits:**
- Surface 429 (rate limit) with toast + exponential backoff
- Surface 402 (payment required) with toast + guidance
- No request queuing for bursts
- No per-tenant token budgets

**Availability & Reliability:**
- Single provider (Lovable AI Gateway)
- Failure ‚Üí fallback to flash-lite with user notice
- No multi-region support
- No provider redundancy

### RECOMMENDED

**Latency Optimization:**
- Pre-warm embeddings for popular queries
- Cache top queries with TTL
- Smaller models for simple tasks (flash-lite < 1s)
- Parallel model invocation for comparison mode

**Cost Structure:**
- Token budget allocation by user tier (free/pro)
- Mix models: cheap for simple, premium for complex
- Monitor cost per session/user
- Alert at 80/90/100% of budget thresholds

**Throughput:**
- Queue bursts with user-facing progress UI
- Per-tenant rate limiting with warnings
- Throttling strategies for high-volume users
- Concurrent request handling optimization

**Availability:**
- Multi-region endpoints (US, EU, APAC)
- Provider redundancy (Lovable AI + direct OpenAI fallback)
- Health checks and automatic failover
- SLA: 99.9% uptime target

## 4) API Integration & Technical Architecture

### CURRENT

**API Compatibility:**
- All requests through Supabase Edge Function: ai-universal-processor
- Lovable AI Gateway (OpenAI-compatible API)
- Streaming SSE supported; client parses line-by-line
- No secrets exposed on client
- Function calling supported (tool use)

**Model Switching Logic:**
- Basic static mapping in ai-universal-processor
- Priority: user_selection > domain > complexity > availability > default
- No dynamic complexity analysis yet
- No confidence-based routing

**Consistency Across Models:**
- Single system prompt enforced server-side
- No tone/personality variations tracked
- Manual markdown formatting (not model-specific)

**Auditability:**
- Log model used, token estimates, routing rationale
- Basic error tracking
- No decision graph visualization

### RECOMMENDED

**Enhanced API Integration:**
- Maintain single request envelope to Lovable AI Gateway
- Support for structured outputs (tool calling)
- Streaming optimization with chunk batching
- Request/response compression for large contexts

**Intelligent Model Switching:**
- Query analyzer to determine optimal model
- Confidence thresholds trigger upshift/downshift
- Decision graph: log why each model was chosen
- A/B testing framework for model comparison

**Cross-Model Consistency:**
- Unified response formatting across models
- Tone/personality alignment system
- Consistent error handling and user messaging
- Standardized citation format from RAG

**Telemetry & Observability:**
- Decision graph (model selection reasoning)
- Latency breakdown (TTFB, streaming, total)
- RAG hit/miss, source counts, confidence scores
- Cost tracking per session/model/user

## 5) Safety & Compliance

### CURRENT

**Safety Filters & Guardrails:**
- Input validation with zod schemas
- Optional content filtering at edge (basic)
- No PII detection/redaction
- No domain-specific content controls

**Data Privacy:**
- No conversation data stored long-term (unless user opts in)
- Secrets stored server-side only
- No custom data retention policies
- GDPR compliance: manual user data deletion

**Auditability:**
- Logs include: model used, token estimates, routing rationale
- Basic error tracking in Supabase logs
- No request replay capability
- No compliance reporting dashboard

### RECOMMENDED

**Enhanced Safety:**
- Fine-grained PII redaction pre-RAG and pre-model
- Domain-specific allowlists (e.g., healthcare terminology only)
- Custom content moderation rules per context
- Automated safety layer for sensitive queries

**Advanced Privacy:**
- Configurable data retention per tenant
- Automated PII scrubbing in logs
- End-to-end encryption for sensitive contexts
- GDPR/HIPAA compliance tooling

**Comprehensive Audit Trail:**
- Full request/response logging with model selection reasoning
- Compliance reporting dashboard
- Request replay for debugging
- Tracking: which model, when, why, for which user/session

## 6) RAG & Knowledge

### CURRENT
- Universal table: universal_knowledge_base with vector + text search
- RAG citations shown in responses when used
- Keyword-based search (ILIKE)
- No confidence scoring

### RECOMMENDED
- Semantic embeddings with vector similarity search
- Confidence scoring and quality weighting
- Per-domain ranking signals for healthcare/tech
- RAG hit/miss tracking and analytics

## 7) UX Flow & Multi-Model Intelligence (Single Source)

### CURRENT

**Key Components:**
- PublicGenieInterface (orchestration)
- SplitScreenRenderer (optional multi-model) - NOT YET OPTIMIZED
- RichResponseRenderer (formatting)
- TopicSuggestionPopover + context detection
- Model badge showing which model was used

**Split-Screen Behavior (Basic):**
- `enableMultiModelComparison` flag ‚Üí shows side-by-side responses
- User can compare Flash vs Pro vs GPT-5 after initial response
- No real-time constraint prioritization yet
- No automatic response format selection

**Constraint Prioritization (Current):**
- Simple priority: User Selection > Domain > Complexity > Default
- No intelligent conflict resolution
- No multi-constraint decision tree
- Example: User selects "Healthcare" + "Vision" + "Flash Lite" ‚Üí No smart handling, just picks Flash Lite

**Model Selection Flow:**

1. **User Selects Model + AI Analyzes Query:**
   - ‚úÖ **Match**: Proceed silently with model badge confirmation
   - üí° **Minor difference**: Gentle toast suggestion, auto-proceed with user choice
   - ‚ö†Ô∏è **Major difference**: Blocking dialog requiring approval (e.g., image on non-vision model)
   - ü§ñ **No selection**: AI chooses automatically, badge shows choice + "Why?" tooltip

2. **Current Scenarios:**
   - No model selected ‚Üí AI recommends silently; badge shows choice + "Why"
   - Minor mismatch (e.g., using Flash for heavy reasoning) ‚Üí gentle toast: "Pro recommended for complex analysis" + continue
   - Major mismatch (e.g., image on Flash Lite) ‚Üí blocking dialog: "Flash Lite doesn't support images. Switch to Gemini Pro?"
   - Match ‚Üí silent proceed with badge

**User Control (Basic):**
- Can manually select any model
- Can override AI recommendations
- No automation level settings yet
- No learning from user choices yet

**Transparency (Basic):**
- Model badge shows which model was used
- No cost/token metrics displayed yet
- No "Why this model?" reasoning visible by default
- No post-response comparison option

### RECOMMENDED

**Enhanced Split-Screen with Intelligent Response Routing:**

Split-screen becomes an intelligent comparison and validation tool:

1. **Real-Time Split-Screen (Advanced):**
   - Left pane: User-selected or AI-recommended primary model
   - Right pane: Alternative model for comparison (optional)
   - Show cost, speed, quality differences side-by-side
   - User can promote either response to "final answer"

2. **Auto-Trigger Split-Screen:**
   - When user selects constraints that conflict (e.g., "Healthcare" + "Vision" + "Fast response")
   - When AI confidence is low (<70%)
   - When cost difference is significant (>50%)
   - When user is in "learning mode"

**Constraint Prioritization Matrix (RECOMMENDED):**

When user selects multiple constraints (LLM type, domain, capabilities), prioritize as follows:

| Priority | Constraint Type | Example | Override Logic |
|----------|----------------|---------|----------------|
| 1 | **Required Capabilities** | Vision, Image Analysis | BLOCKS incompatible models |
| 2 | **Domain Context** | Healthcare, Clinical, Research | STRONG preference for domain-specific models |
| 3 | **User Explicit Selection** | "Use GPT-5" | HONORED unless capability conflict (Priority 1) |
| 4 | **Performance Requirements** | Fast, Low-Cost | SUGGESTS alternatives, doesn't block |
| 5 | **Default/Fallback** | gemini-2.5-flash | APPLIES when no other constraints |

**Constraint Resolution Decision Tree:**

```
User Input: "Analyze this medical scan" + Context: Healthcare + Selection: Flash Lite
     ‚Üì
Step 1: Required Capabilities Check
     ‚Üì
   Vision Required? ‚Üí YES
   Flash Lite supports vision? ‚Üí NO
     ‚Üì
‚ùå BLOCK: Flash Lite incompatible
     ‚Üì
Step 2: Domain Context Check
     ‚Üì
   Domain: Healthcare ‚Üí Prefer Pro (better medical accuracy)
     ‚Üì
Step 3: User Selection Check
     ‚Üì
   User selected Flash Lite ‚Üí Conflict with Step 1 & 2
     ‚Üì
Step 4: Resolution
     ‚Üì
‚ö†Ô∏è Blocking Dialog:
"Flash Lite doesn't support vision and is suboptimal for healthcare.
 RECOMMENDED: Gemini Pro
 - Vision: ‚úÖ  Healthcare accuracy: ‚úÖ  Cost: $0.02
 ALTERNATIVE: Gemini Flash (vision: ‚úÖ, cost: $0.01, accuracy: moderate)
 
 Proceed with Pro? [Yes] [Use Flash] [Cancel]"
```

**Enhanced Model Selection Flow:**

```
User Input ‚Üí [Parse All Constraints]
     ‚Üì
Required Capabilities (Vision/RAG/Multi-model)?
     ‚Üì YES                           ‚Üì NO
Filter compatible models        All models available
     ‚Üì                           ‚Üì
Domain Context (Healthcare/Tech/Research)?
     ‚Üì YES                           ‚Üì NO
Prefer domain-optimized         General models OK
     ‚Üì                           ‚Üì
User Explicit Selection?
     ‚Üì YES                           ‚Üì NO
Check compatibility             AI Auto-Select
     ‚Üì                           ‚Üì
Compatible? ‚Üí Proceed           Show reasoning badge
Not compatible? ‚Üí Block dialog
     ‚Üì
[Decision Tree Outcome]
```

**Decision Tree for User-Selected Models:**

| Scenario | AI Recommendation | User Selection | Action | UI Feedback |
|----------|-------------------|----------------|--------|-------------|
| ‚úÖ Match | Flash | Flash | Proceed silently | Green badge: "Gemini Flash (optimal)" |
| üí° Minor Diff | Pro | Flash | Toast + Auto-proceed | Orange toast: "Pro recommended for better results. Continuing with Flash..." |
| ‚ö†Ô∏è Major Diff | Pro (vision) | Flash Lite | Block + Require approval | Dialog: "Flash Lite doesn't support images. Switch to Pro?" |
| ü§ñ No Selection | - | - | AI auto-selects | Badge: "Gemini Flash (auto-selected)" + "Why?" tooltip |

**User Automation Control (Settings):**

Three automation levels users can configure:

1. **Manual Mode** (User always chooses):
   - No AI suggestions
   - User picks model every time
   - AI shows post-response what it would have chosen

2. **Smart Assist** (Default - AI suggests, user controls):
   - AI analyzes and suggests
   - Gentle toasts for minor differences
   - Blocking dialogs for major mismatches
   - User can override any suggestion
   - System learns from overrides

3. **Auto-Optimize** (AI chooses, user monitors):
   - AI auto-selects optimal model
   - Silent model switching
   - Badge shows reasoning
   - User can manually override on next query
   - System learns from manual overrides

**System Learning (Recommended):**
- Track user model preferences per query type
- Learn patterns: "User prefers Pro for medical queries despite Flash being suggested"
- Adjust AI suggestions based on user history
- Privacy: Learning stored per-user session only

**Enhanced Transparency (Every Response):**

Display in response footer or expandable card:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ü§ñ Model: Gemini 2.5 Flash                     ‚îÇ
‚îÇ üí° Why: Balanced speed + quality for general Q  ‚îÇ
‚îÇ üí∞ Cost: ~$0.006 (estimated)                    ‚îÇ
‚îÇ üéØ Tokens: 1,523 / 3,000 budget                ‚îÇ
‚îÇ ‚ö° Speed: 1.8s response time                    ‚îÇ
‚îÇ üìä [Compare with other models]                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Post-Response Comparison (Recommended):**
- "Compare with other models" button
- Side-by-side: Flash vs Pro vs GPT-5
- Shows quality difference, cost difference, speed difference
- Helps user learn which model to prefer
- Enable `enableMultiModelComparison` flag

**Example Flow:**

```
User: [Uploads medical scan] "What does this show?"
User Selection: Flash Lite (trying to save costs)
     ‚Üì
AI Analysis: 
- Detects image
- Requires vision capability
- Flash Lite doesn't support vision
     ‚Üì
‚ö†Ô∏è Blocking Dialog:
"Flash Lite doesn't support images. 
Switch to Gemini Pro for image analysis?
Estimated cost: $0.02 vs $0.005"
     ‚Üì
User: [Approves switch]
     ‚Üì
Response Badge:
"ü§ñ Gemini Pro (switched from Flash Lite)
üí° Why: Image analysis requires vision capability
üí∞ Cost: $0.02 | ‚ö° 2.3s"
```

### UX Flow Diagram (Enhanced)

```
flowchart TD
    A[User input + optional model selection] --> B{User selected model?}
    B -- Yes --> C[AI analyzes query requirements]
    B -- No --> D[AI analyzes + auto-selects]
    
    C --> E{Compare user vs AI choice}
    E -- Match --> F[‚úÖ Proceed silently + badge]
    E -- Minor diff --> G[üí° Toast suggestion + auto-proceed]
    E -- Major diff --> H[‚ö†Ô∏è Blocking dialog + require approval]
    
    D --> I[Show auto-selected model + reasoning badge]
    
    F --> J[Invoke RAG if enabled]
    G --> J
    H --> K{User approves?}
    K -- Yes --> J
    K -- No --> L[Use user's original choice]
    I --> J
    L --> J
    
    J --> M[Stream response + show metrics]
    M --> N[Display: Model badge, cost, tokens, speed]
    N --> O{Show outcome analytics?}
    O -- Yes --> P[Compare with other models option]
    O -- No --> Q[End]
    P --> Q
```

**Settings UI (Recommended):**
```typescript
interface GeniePreferences {
  automationLevel: 'manual' | 'smart-assist' | 'auto-optimize';
  showCostMetrics: boolean;
  showPerformanceMetrics: boolean;
  enablePostResponseComparison: boolean;
  defaultModel?: string; // User's preferred default
  learnFromChoices: boolean; // Privacy toggle
}
```

## 8) Feature Flags and Behavior

**Current Flags:**
- enableSmartRouting: false (default)
- enableMultiModelComparison: false (default)
- showOutcomeAnalytics: false (default)
- Enforcement: All flags are additive; switching on any feature must not break base flow

**Recommended New Flags:**
- automationLevel: 'smart-assist' (default) | 'manual' | 'auto-optimize'
- showCostMetrics: false (default, enable in Phase 4)
- showPerformanceMetrics: false (default, enable in Phase 4)
- enableSystemLearning: false (default, enable in Phase 6)
- enablePostResponseComparison: false (default, enable with multi-model)


## 9) Implementation Guide (Step-by-Step)

### Phase 1: Query Intelligence (Week 1-2)
**Goal:** Understand what users are asking

**Files to Create:**
- `supabase/functions/ai-query-analyzer/index.ts` - Analyzes complexity, domain, intent, required capabilities
- `src/services/queryAnalysisService.ts` - Frontend query preprocessing
- `src/types/aiQuery.ts` - Type definitions

**Features:**
1. Intent classification (research, clinical, general, technical)
2. Domain detection (healthcare, tech, multi-domain)
3. Complexity scoring (0-100)
4. Required capabilities detection (vision, RAG, multi-model)
5. Cost estimation

**No user-facing changes** - collect data and log analysis results only.

### Phase 2: Model Selection Enhancement (Week 2-3)
**Goal:** Smart routing with user control and transparency

**Files to Modify:**
- `supabase/functions/ai-universal-processor/index.ts` - Add intelligent routing with comparison logic
- `src/components/public-genie/ModelSelectionDialog.tsx` - Create blocking dialog for major mismatches
- `src/components/public-genie/ModelBadge.tsx` - Enhanced badge with reasoning tooltip

**Decision Matrix Implementation:**
```typescript
interface ModelComparisonResult {
  userChoice: string;
  aiRecommendation: string;
  matchLevel: 'perfect' | 'minor' | 'major';
  reasoning: string;
  action: 'proceed' | 'suggest' | 'block';
}

function compareUserVsAIChoice(
  userModel: string | undefined,
  aiAnalysis: QueryAnalysis
): ModelComparisonResult {
  // Implementation of match/minor/major logic
}
```

**UX Components:**
- ‚úÖ Match: Silent proceed with green badge
- üí° Minor: Toast with suggestion, auto-proceed
- ‚ö†Ô∏è Major: Blocking dialog requiring approval
- ü§ñ No selection: Auto-select with reasoning badge

**Feature flag:** `enableSmartRouting=false` (test internally first)

### Phase 2.5: Multi-Model & RAG Decision Logic (Week 3)
**Goal:** Intelligent decision when to use single vs multi-model with RAG/Knowledge Base

**Files to Create:**
- `src/utils/multiModelDecisionEngine.ts` - Logic for single vs multi-model responses
- `src/types/modelDecision.ts` - Decision types and interfaces

**Decision Logic: Single vs Multi-Model Response**

```typescript
interface ResponseStrategy {
  mode: 'single' | 'multi-model' | 'ensemble';
  models: string[];
  useRAG: boolean;
  useKnowledgeBase: boolean;
  reasoning: string;
}

function determineResponseStrategy(
  query: string,
  userConstraints: UserConstraints,
  context: ConversationContext
): ResponseStrategy {
  // Priority 1: Required capabilities filter
  const compatibleModels = filterByCapabilities(userConstraints);
  
  // Priority 2: Domain context influences choice
  const domainOptimal = getDomainOptimalModels(userConstraints.domain);
  
  // Priority 3: Complexity determines single vs multi
  const complexity = analyzeComplexity(query);
  
  if (complexity.requiresComparison || userConstraints.showComparison) {
    return {
      mode: 'multi-model',
      models: [domainOptimal.primary, domainOptimal.alternative],
      useRAG: shouldUseRAG(query, context),
      useKnowledgeBase: shouldUseKnowledgeBase(userConstraints.domain),
      reasoning: 'Comparison requested or high complexity detected'
    };
  }
  
  if (complexity.requiresEnsemble) {
    return {
      mode: 'ensemble',
      models: [domainOptimal.primary, 'gemini-2.5-flash'], // Primary + Fast
      useRAG: true,
      useKnowledgeBase: true,
      reasoning: 'Combining multiple models for validation'
    };
  }
  
  return {
    mode: 'single',
    models: [domainOptimal.primary],
    useRAG: shouldUseRAG(query, context),
    useKnowledgeBase: shouldUseKnowledgeBase(userConstraints.domain),
    reasoning: 'Single model sufficient for query'
  };
}
```

**RAG & Knowledge Base Priority:**

```typescript
// Determine when to use RAG vs Knowledge Base vs Both
function shouldUseRAG(query: string, context: ConversationContext): boolean {
  // Use RAG when:
  // - Query mentions specific facts, citations, research
  // - Context indicates user wants sources
  // - Domain is scientific/research
  return (
    query.includes('research') ||
    query.includes('study') ||
    query.includes('citation') ||
    context.domain === 'research' ||
    context.requiresSources
  );
}

function shouldUseKnowledgeBase(domain: string): boolean {
  // Use Knowledge Base when:
  // - Domain is healthcare, clinical, or technology
  // - User has selected domain context
  return ['healthcare', 'clinical', 'technology', 'research'].includes(domain);
}

function getCombinedStrategy(
  query: string,
  domain: string,
  context: ConversationContext
): { useRAG: boolean; useKnowledgeBase: boolean; priority: string } {
  const ragNeeded = shouldUseRAG(query, context);
  const kbNeeded = shouldUseKnowledgeBase(domain);
  
  if (ragNeeded && kbNeeded) {
    return {
      useRAG: true,
      useKnowledgeBase: true,
      priority: 'RAG first, then KB supplement' // RAG has precedence for sources
    };
  }
  
  if (ragNeeded) {
    return { useRAG: true, useKnowledgeBase: false, priority: 'RAG only' };
  }
  
  if (kbNeeded) {
    return { useRAG: false, useKnowledgeBase: true, priority: 'KB only' };
  }
  
  return { useRAG: false, useKnowledgeBase: false, priority: 'Model only' };
}
```

**Example Decision Trees:**

**Scenario 1: User selects "Healthcare" + "Vision" + "Fast"**
```
Step 1: Filter by capabilities ‚Üí Vision required ‚Üí Filter: [Pro, GPT-5]
Step 2: Domain priority ‚Üí Healthcare ‚Üí Prefer: Pro (domain-optimized)
Step 3: Performance ‚Üí Fast requested ‚Üí Conflict!
Step 4: Resolution ‚Üí 
  PRIMARY: Gemini Pro (domain + vision)
  TOAST: "Pro recommended for healthcare image analysis. Flash doesn't support vision. Estimated: 2.3s"
  MODE: Single model + Knowledge Base (healthcare context)
```

**Scenario 2: User asks "Compare research studies on X"**
```
Step 1: Capabilities ‚Üí Text only ‚Üí All models OK
Step 2: Domain ‚Üí Research ‚Üí Use RAG + Knowledge Base
Step 3: Intent ‚Üí "Compare" keyword ‚Üí Multi-model mode
Step 4: Resolution ‚Üí
  PRIMARY: Gemini Pro (research analysis)
  SECONDARY: GPT-5 (alternative perspective)
  MODE: Multi-model + RAG (priority) + KB (supplement)
  SPLIT-SCREEN: Show both responses side-by-side
  REASONING: "Comparison mode: Pro for depth, GPT-5 for alternative analysis"
```

**Scenario 3: User selects "Clinical" + "Small Language Model"**
```
Step 1: Capabilities ‚Üí Text only ‚Üí SLM compatible
Step 2: Domain ‚Üí Clinical ‚Üí Conflict! (Clinical requires accuracy, SLM may be insufficient)
Step 3: Resolution ‚Üí
  ‚ö†Ô∏è WARNING DIALOG:
  "Small Language Models may not provide clinical-grade accuracy.
   RECOMMENDED: Gemini Pro for clinical queries
   ALTERNATIVE: Gemini Flash (moderate accuracy, lower cost)
   CONTINUE with SLM? (Not recommended) [Switch to Pro] [Use Flash] [Continue anyway]"
```

### Phase 3: User Preferences & Automation Levels (Week 3-4)
**Goal:** Give users control over automation

**Files to Create:**
- `src/components/public-genie/GenieSettings.tsx` - Settings panel
- `src/hooks/useGeniePreferences.ts` - Preference management hook
- Database table: `genie_user_preferences`

**Automation Levels:**
1. **Manual Mode**: User always chooses, AI shows what it would have picked
2. **Smart Assist** (default): AI suggests with toasts/dialogs, user controls
3. **Auto-Optimize**: AI chooses automatically, user can monitor

**Settings Schema:**
```typescript
interface GeniePreferences {
  user_id: string;
  automation_level: 'manual' | 'smart-assist' | 'auto-optimize';
  show_cost_metrics: boolean;
  show_performance_metrics: boolean;
  enable_post_response_comparison: boolean;
  default_model?: string;
  learn_from_choices: boolean;
}
```

### Phase 4: Transparency & Metrics Display (Week 4-5)
**Goal:** Show users cost, tokens, performance for every response

**Files to Create:**
- `src/components/public-genie/ResponseMetrics.tsx` - Metrics card component
- `src/utils/costCalculator.ts` - Calculate cost estimates

**Metrics to Display:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ü§ñ Model: Gemini 2.5 Flash                     ‚îÇ
‚îÇ üí° Why: Balanced speed + quality for general Q  ‚îÇ
‚îÇ üí∞ Cost: ~$0.006 (estimated)                    ‚îÇ
‚îÇ üéØ Tokens: 1,523 / 3,000 budget                ‚îÇ
‚îÇ ‚ö° Speed: 1.8s response time                    ‚îÇ
‚îÇ üìä [Compare with other models]                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Feature flags:**
- `showCostMetrics=true`
- `showPerformanceMetrics=true`

### Phase 5: Post-Response Comparison (Week 5-6)
**Goal:** Let users compare how different models would have responded

**Files to Create:**
- `src/components/public-genie/ModelComparisonView.tsx` - Side-by-side comparison UI
- `supabase/functions/ai-multi-model-comparison/index.ts` - Invoke multiple models in parallel

**Features:**
- "Compare with other models" button on each response
- Side-by-side view: Flash vs Pro vs GPT-5
- Show quality, cost, speed differences
- Help users learn which model to prefer

**Feature flag:** `enableMultiModelComparison=true`

### Phase 5.5: Token Optimization & Context Management (Week 6)
**Goal:** Optimize token usage based on query type, domain, and model selection while maintaining quality

**Files to Create:**
- `src/utils/tokenOptimizer.ts` - Dynamic token allocation logic
- `src/utils/contextManager.ts` - Context compression and history management

**Token Optimization by Domain & Context:**

```typescript
interface TokenAllocation {
  maxTokens: number;
  contextWindow: number;
  compressionStrategy: 'none' | 'summarize' | 'truncate';
  reasoning: string;
}

const TOKEN_OPTIMIZATION_BY_DOMAIN = {
  healthcare: {
    simple: { maxTokens: 2000, context: 8000, compression: 'none' },
    moderate: { maxTokens: 4000, context: 16000, compression: 'summarize' },
    complex: { maxTokens: 8000, context: 32000, compression: 'none' },
    reasoning: 'Healthcare requires precision; avoid aggressive compression'
  },
  research: {
    simple: { maxTokens: 3000, context: 12000, compression: 'none' },
    moderate: { maxTokens: 6000, context: 24000, compression: 'summarize' },
    complex: { maxTokens: 12000, context: 50000, compression: 'none' },
    reasoning: 'Research needs extensive context for citations and analysis'
  },
  general: {
    simple: { maxTokens: 1500, context: 4000, compression: 'truncate' },
    moderate: { maxTokens: 3000, context: 8000, compression: 'summarize' },
    complex: { maxTokens: 6000, context: 16000, compression: 'summarize' },
    reasoning: 'General queries can use aggressive compression for cost savings'
  },
  clinical: {
    simple: { maxTokens: 2500, context: 10000, compression: 'none' },
    moderate: { maxTokens: 5000, context: 20000, compression: 'none' },
    complex: { maxTokens: 10000, context: 40000, compression: 'none' },
    reasoning: 'Clinical context cannot be compressed; patient safety critical'
  },
  technology: {
    simple: { maxTokens: 2000, context: 6000, compression: 'truncate' },
    moderate: { maxTokens: 4000, context: 12000, compression: 'summarize' },
    complex: { maxTokens: 8000, context: 24000, compression: 'summarize' },
    reasoning: 'Tech queries benefit from context but can handle summarization'
  }
};

function optimizeTokenAllocation(
  query: string,
  domain: string,
  model: string,
  conversationHistory: Message[]
): TokenAllocation {
  const complexity = analyzeQueryComplexity(query);
  const domainRules = TOKEN_OPTIMIZATION_BY_DOMAIN[domain] || TOKEN_OPTIMIZATION_BY_DOMAIN.general;
  
  let allocation = domainRules[complexity.level]; // simple/moderate/complex
  
  // Adjust for model capabilities
  if (model.includes('flash-lite')) {
    allocation.maxTokens = Math.min(allocation.maxTokens, 2000); // Cap for lite models
    allocation.compression = 'truncate'; // Aggressive compression for lite
  }
  
  if (model.includes('gpt-5')) {
    allocation.contextWindow = Math.min(allocation.contextWindow, 100000); // GPT-5 limit
  }
  
  // Adjust for conversation history length
  const historyTokens = estimateHistoryTokens(conversationHistory);
  if (historyTokens > allocation.contextWindow * 0.7) {
    allocation.compression = domain === 'clinical' ? 'summarize' : 'truncate';
  }
  
  return {
    ...allocation,
    reasoning: `${domainRules.reasoning}. Complexity: ${complexity.level}. Model: ${model}`
  };
}

// Context compression strategies
function compressContext(
  history: Message[],
  strategy: 'none' | 'summarize' | 'truncate',
  targetTokens: number
): Message[] {
  if (strategy === 'none') return history;
  
  if (strategy === 'truncate') {
    // Keep only recent messages that fit in target tokens
    let tokens = 0;
    const compressed = [];
    for (let i = history.length - 1; i >= 0; i--) {
      const msgTokens = estimateTokens(history[i].content);
      if (tokens + msgTokens > targetTokens) break;
      compressed.unshift(history[i]);
      tokens += msgTokens;
    }
    return compressed;
  }
  
  if (strategy === 'summarize') {
    // Keep first message, summarize middle, keep last 5
    const first = history[0];
    const last5 = history.slice(-5);
    const middle = history.slice(1, -5);
    
    if (middle.length === 0) return history;
    
    // Call AI to summarize middle (async, cache result)
    const summary = {
      role: 'system',
      content: `[Previous conversation summarized: ${middle.length} messages about ${extractTopics(middle)}]`
    };
    
    return [first, summary, ...last5];
  }
}
```

**Token Optimization Decision Matrix:**

| Domain | Complexity | Model | Max Tokens | Context Window | Compression | Cost Est. |
|--------|-----------|-------|------------|----------------|-------------|-----------|
| Healthcare | Simple | Flash | 2000 | 8000 | None | $0.008 |
| Healthcare | Complex | Pro | 8000 | 32000 | None | $0.04 |
| Research | Moderate | Pro | 6000 | 24000 | Summarize | $0.03 |
| General | Simple | Flash Lite | 1500 | 4000 | Truncate | $0.003 |
| Clinical | Complex | Pro | 10000 | 40000 | None | $0.05 |
| Technology | Moderate | Flash | 4000 | 12000 | Summarize | $0.012 |

**Context Preservation Strategies:**

```typescript
// Maintain context quality while optimizing tokens
function maintainContextQuality(
  query: string,
  domain: string,
  history: Message[],
  allocation: TokenAllocation
): { messages: Message[], preserved: string[], dropped: string[] } {
  
  const critical = extractCriticalContext(query, history, domain);
  const preserved = [];
  const dropped = [];
  
  // Always preserve:
  // 1. System prompt
  // 2. Critical domain context (e.g., patient info for clinical)
  // 3. Last 3 user messages
  // 4. Any referenced prior responses
  
  let tokenBudget = allocation.contextWindow;
  
  // Reserve tokens for query and response
  tokenBudget -= estimateTokens(query);
  tokenBudget -= allocation.maxTokens; // Reserve for response
  
  // Add critical context first
  for (const msg of critical) {
    const tokens = estimateTokens(msg.content);
    if (tokenBudget - tokens > 0) {
      preserved.push(msg.id);
      tokenBudget -= tokens;
    } else {
      dropped.push(msg.id);
    }
  }
  
  // Add recent history
  for (let i = history.length - 1; i >= 0; i--) {
    if (preserved.includes(history[i].id)) continue;
    const tokens = estimateTokens(history[i].content);
    if (tokenBudget - tokens > 0) {
      preserved.push(history[i].id);
      tokenBudget -= tokens;
    } else {
      dropped.push(history[i].id);
    }
  }
  
  return {
    messages: history.filter(m => preserved.includes(m.id)),
    preserved,
    dropped
  };
}
```

### Phase 5.6: Response Format Intelligence (Week 6)
**Goal:** Automatically determine optimal response format based on query type and context

**Files to Create:**
- `src/utils/responseFormatSelector.ts` - Logic to determine response format
- `src/components/public-genie/AdaptiveResponseRenderer.tsx` - Dynamic format rendering

**Response Format Selection Logic:**

```typescript
type ResponseFormat = 'text' | 'table' | 'image' | 'html' | 'video' | 'mixed';

interface FormatDecision {
  primary: ResponseFormat;
  fallback: ResponseFormat;
  reasoning: string;
  toolsRequired?: string[];
}

function determineResponseFormat(
  query: string,
  domain: string,
  context: ConversationContext
): FormatDecision {
  
  // Priority 1: Explicit format requests
  if (query.match(/show.*table|tabular|spreadsheet/i)) {
    return {
      primary: 'table',
      fallback: 'text',
      reasoning: 'User explicitly requested tabular format'
    };
  }
  
  if (query.match(/diagram|chart|visualiz|image|picture/i)) {
    return {
      primary: 'image',
      fallback: 'text',
      reasoning: 'User requested visual representation',
      toolsRequired: ['image-generation']
    };
  }
  
  if (query.match(/video|animation|demo/i)) {
    return {
      primary: 'video',
      fallback: 'image',
      reasoning: 'User requested video/animation',
      toolsRequired: ['video-generation'] // Future capability
    };
  }
  
  // Priority 2: Domain-specific patterns
  if (domain === 'research' && query.match(/compar|vs|versus|between/i)) {
    return {
      primary: 'table',
      fallback: 'text',
      reasoning: 'Research comparison best shown in table format'
    };
  }
  
  if (domain === 'healthcare' && query.match(/scan|image|x-ray|mri|ct/i)) {
    return {
      primary: 'image',
      fallback: 'mixed',
      reasoning: 'Medical imaging query requires visual + text',
      toolsRequired: ['vision-analysis']
    };
  }
  
  if (domain === 'clinical' && query.match(/timeline|progression|history/i)) {
    return {
      primary: 'html',
      fallback: 'table',
      reasoning: 'Clinical timeline best shown with interactive HTML'
    };
  }
  
  // Priority 3: Query intent analysis
  const intent = analyzeIntent(query);
  
  if (intent.type === 'comparison' && intent.entities > 2) {
    return {
      primary: 'table',
      fallback: 'text',
      reasoning: 'Multiple entity comparison optimal in table'
    };
  }
  
  if (intent.type === 'procedural' || intent.type === 'tutorial') {
    return {
      primary: 'html',
      fallback: 'text',
      reasoning: 'Step-by-step content works best with formatted HTML'
    };
  }
  
  // Default: text
  return {
    primary: 'text',
    fallback: 'text',
    reasoning: 'Standard text response sufficient'
  };
}
```

**Response Format by Domain (Reference Table):**

| Query Type | Domain | Optimal Format | Example Query | Why |
|-----------|--------|----------------|---------------|-----|
| Comparison | Research | Table | "Compare studies A vs B vs C" | Multi-column data |
| Imaging | Healthcare | Image + Text | "What does this MRI show?" | Visual analysis required |
| Timeline | Clinical | HTML/Interactive | "Patient progression over 6 months" | Temporal visualization |
| Data Analysis | Technology | Table + Chart | "Analyze these metrics" | Structured data view |
| Tutorial | General | HTML + Video | "How to configure X?" | Step-by-step with visuals |
| Q&A | All | Text | "What is X?" | Simple text sufficient |
| Statistical | Research | Table + Text | "Show statistical significance" | Numbers + interpretation |
| Diagnostic | Clinical | Mixed (Image + Table + Text) | "Review lab results and scan" | Multi-modal analysis |

**Format Rendering Strategy:**

```typescript
// Dynamic component selection based on format decision
function renderResponse(
  content: string,
  format: ResponseFormat,
  domain: string
) {
  switch (format) {
    case 'text':
      return <RichResponseRenderer content={content} />;
    
    case 'table':
      return <TableRenderer data={parseTableData(content)} domain={domain} />;
    
    case 'image':
      return (
        <ImageResponseRenderer 
          images={extractImages(content)} 
          caption={extractCaption(content)}
          domain={domain}
        />
      );
    
    case 'html':
      return (
        <InteractiveHTMLRenderer 
          html={sanitizeHTML(content)} 
          domain={domain}
          allowedTags={getDomainAllowedTags(domain)}
        />
      );
    
    case 'mixed':
      return (
        <MixedFormatRenderer 
          sections={parseMultiFormatContent(content)}
          domain={domain}
        />
      );
    
    default:
      return <RichResponseRenderer content={content} />;
  }
}
```

**Tool Calling for Format-Specific Responses:**

```typescript
// When response requires specific format, use tool calling
const formatTools = {
  'image-generation': {
    type: 'function',
    function: {
      name: 'generate_image',
      description: 'Generate an image based on description',
      parameters: {
        type: 'object',
        properties: {
          description: { type: 'string' },
          style: { type: 'string', enum: ['medical', 'diagram', 'chart', 'realistic'] }
        }
      }
    }
  },
  'table-generation': {
    type: 'function',
    function: {
      name: 'generate_table',
      description: 'Generate a structured table',
      parameters: {
        type: 'object',
        properties: {
          headers: { type: 'array', items: { type: 'string' } },
          rows: { type: 'array', items: { type: 'array' } }
        }
      }
    }
  }
};

// Include tools in AI request based on format decision
function buildAIRequest(
  query: string,
  formatDecision: FormatDecision,
  model: string
) {
  const baseRequest = {
    model,
    messages: [{ role: 'user', content: query }]
  };
  
  if (formatDecision.toolsRequired) {
    baseRequest.tools = formatDecision.toolsRequired.map(
      tool => formatTools[tool]
    );
    baseRequest.tool_choice = 'auto';
  }
  
  return baseRequest;
}
```

**Feature flag:** `enableMultiModelComparison=true`, `enableAdaptiveFormatting=false` (Phase 6)

### Phase 6: System Learning (Week 6-7)
**Goal:** Learn from user choices to improve recommendations

**Files to Create:**
- Database table: `genie_model_choices` - Log user selections vs AI recommendations
- `src/utils/modelLearning.ts` - Pattern detection and preference learning

**Learning Logic:**
```typescript
interface UserModelPattern {
  user_id: string;
  query_type: string;
  ai_recommended: string;
  user_chose: string;
  frequency: number;
  last_seen: timestamp;
}

// Example: User always picks Pro for medical queries despite AI suggesting Flash
// ‚Üí Future medical queries: AI auto-suggests Pro
```

**Privacy:**
- Learning stored per-user session only
- Can be disabled via `learn_from_choices` setting
- No cross-user learning (privacy-preserving)

**Feature flag:** `enableSystemLearning=false` (enable after testing)

### Client Integration Points
- Hook: `src/hooks/useUniversalAI.ts`
  - Add model comparison logic
  - Track user choices vs AI recommendations
  - Show model badge with reasoning
- Component: `PublicGenieInterface`
  - Integrate settings panel
  - Show metrics card
  - Handle blocking dialogs

### UX Instrumentation (Enhanced)
- Always show typing indicator while streaming
- Model badge with reasoning tooltip on every response
- Toast notifications for suggestions (minor mismatches)
- Blocking dialogs for major mismatches
- Metrics card (expandable/collapsible)
- Settings accessible from Genie UI
- Surface 429 ‚Üí "Rate limit hit; retrying" toast with backoff
- Surface 402 ‚Üí "Workspace credits required" toast
- On auto-fallback, show subtle notice badge

### Rollout Coordination
- Phase 1-2: Internal testing (baseline + smart routing)
- Phase 3-4: QA with user preferences + transparency
- Phase 5: Limited beta for multi-model comparison
- Phase 6: GA with all features, learning disabled by default
- See Ops Runbook (docs/Ops_Runbook_Genie.md) for SLO targets and monitoring

## 10) Strategy & Roadmap

### Near-term
- Keep Flash default, Pro for vision/complex; instrument routing rationale and latency
- Enable query intelligence with intent classification, domain detection, complexity scoring
- Add basic token optimization: dynamic allocation based on query complexity

### Mid-term
- Confidence-triggered upshift, token-aware caps, SLM pre-summarization
- RAG enhancement with semantic embeddings and confidence scoring
- Response format intelligence with auto-detection and tool calling
- Multi-model orchestration for side-by-side comparison

### Long-term
- Provider redundancy, multi-region endpoints for availability
- Adaptive memory with summarized history and deduped context
- Advanced cost optimization per tier
- Interactive response components (charts, diagrams, tables)

### Expected Performance Impact
| Metric | Current | After Optimization | Improvement |
|--------|---------|-------------------|-------------|
| Avg Response Time | 3-5s | 1.5-3s | 40-50% faster |
| Token Usage (Simple) | 4000 | 1500 | 62% reduction |
| Token Usage (Complex) | 4000 | 6000 | Better quality |
| RAG Accuracy | ~60% | ~85% | 25% improvement |
| Cost per Query (Simple) | $0.02 | $0.006 | 70% reduction |
| User Satisfaction | Baseline | +40% | Smarter responses |

## 11) Future-Proofing & Scalability

### CURRENT
- Single provider (Lovable AI Gateway)
- No model versioning strategy
- No abstraction layer for provider swapping
- Manual model updates in code

### RECOMMENDED

**Model Versioning Strategy:**
- Central model registry with versions and deprecation dates
- Gradual rollout: test new models on small cohort first
- Backward compatibility: maintain old model endpoints during migration
- Feature flags per model version

**Vendor Lock-in Mitigation:**
- Abstraction layer: unified interface for multiple providers
- Provider fallback chains (Lovable AI ‚Üí OpenAI direct ‚Üí Anthropic)
- Standard request/response envelope across providers
- Easy provider swapping via configuration

**Scalability:**
- Horizontal scaling: add models/providers without code changes
- Auto-scaling based on request volume
- Model load balancing across regions
- Caching layer for repeated queries

## 12) Appendices

### A. Model Priority (authoritative)
user_selection > domain_requirement > complexity/cost > availability > default

### B. Task-Specific Model Recommendations

| Task Type | Primary Model | Fallback | Reasoning |
|-----------|---------------|----------|-----------|
| General conversation | gemini-2.5-flash | flash-lite | Balanced cost/quality |
| Healthcare/Clinical | gemini-2.5-pro | flash | Domain accuracy critical |
| Code generation | gemini-2.5-flash | gpt-5-mini | Adequate for most code |
| Creative writing | gemini-2.5-pro | gpt-5 | Nuance matters |
| Analytical reasoning | openai/gpt-5 | gemini-2.5-pro | Complex multi-step |
| Quick classification | gemini-2.5-flash-lite | flash | Speed over precision |
| Vision tasks | gemini-2.5-pro | N/A | Multimodal required |

### C. Context Window Guidelines

| Model | Max Context | Practical Limit | Use Case |
|-------|-------------|-----------------|----------|
| Gemini 2.5 Pro | 2M tokens | 100K tokens | Long documents, extensive history |
| Gemini 2.5 Flash | 1M tokens | 50K tokens | Standard conversations |
| Gemini 2.5 Flash Lite | 1M tokens | 20K tokens | Quick queries |
| OpenAI GPT-5 | 200K tokens | 100K tokens | Complex reasoning |

### D. Error Surfacing (authoritative)
- 429 Too Many Requests ‚Üí toast: "Rate limit hit; retrying"; auto-backoff
- 402 Payment Required ‚Üí toast: "Workspace credits required"
- Model unavailable ‚Üí silent fallback + subtle notice badge
- Confidence low ‚Üí optional model switch suggestion

---
This playbook supersedes prior documents and is the canonical reference for AI routing, RAG, UX behavior, and phased rollout.
